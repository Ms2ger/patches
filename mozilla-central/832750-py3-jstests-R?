From: Ms2ger <ms2ger@gmail.com>

diff --git a/js/src/tests/jstests.py b/js/src/tests/jstests.py
--- a/js/src/tests/jstests.py
+++ b/js/src/tests/jstests.py
@@ -1,15 +1,17 @@
 #!/usr/bin/env python
 """
 The JS Shell Test Harness.
 
 See the adjacent README.txt for more details.
 """
 
+from __future__ import print_function
+
 import os, sys, textwrap
 from os.path import abspath, dirname, realpath
 from copy import copy
 from subprocess import list2cmdline, call
 
 from lib.results import NullTestOutput
 from lib.tests import TestCase
 from lib.results import ResultsSink
@@ -189,17 +191,17 @@ def parse_args():
 
     # Handle output redirection, if requested and relevant.
     options.output_fp = sys.stdout
     if options.output_file:
         if not options.show_cmd:
             options.show_output = True
         try:
             options.output_fp = open(options.output_file, 'w')
-        except IOError, ex:
+        except IOError as ex:
             raise SystemExit("Failed to open output file: " + str(ex))
 
     options.show = options.show_cmd or options.show_output
 
     # Hide the progress bar if it will get in the way of other output.
     options.hide_progress = (options.tinderbox or
                              not ProgressBar.conservative_isatty() or
                              options.hide_progress)
@@ -292,31 +294,31 @@ def load_tests(options, requested_paths,
 
     return skip_list, test_list
 
 def main():
     options, requested_paths, excluded_paths = parse_args()
     skip_list, test_list = load_tests(options, requested_paths, excluded_paths)
 
     if not test_list:
-        print 'no tests selected'
+        print('no tests selected')
         return 1
 
     test_dir = dirname(abspath(__file__))
 
     if options.debug:
         if len(test_list) > 1:
             print('Multiple tests match command line arguments, debugger can only run one')
             for tc in test_list:
                 print('    %s'%tc.path)
             return 2
 
         cmd = test_list[0].get_command(TestCase.js_cmd_prefix)
         if options.show_cmd:
-            print list2cmdline(cmd)
+            print(list2cmdline(cmd))
         if test_dir not in ('', '.'):
             os.chdir(test_dir)
         call(cmd)
         return 0
 
     curdir = os.getcwd()
     if test_dir not in ('', '.'):
         os.chdir(test_dir)
diff --git a/js/src/tests/lib/manifest.py b/js/src/tests/lib/manifest.py
--- a/js/src/tests/lib/manifest.py
+++ b/js/src/tests/lib/manifest.py
@@ -1,13 +1,15 @@
 # Library for JSTest manifests.
 #
 # This includes classes for representing and parsing JS manifests.
 
-import os, os.path, re, sys
+from __future__ import print_function
+
+import os, re, sys
 from subprocess import Popen, PIPE
 
 from tests import TestCase
 
 
 def split_path_into_dirs(path):
     dirs = [path]
 
@@ -50,18 +52,18 @@ class XULInfo:
         path = None
         for dir in dirs:
             _path = os.path.join(dir, 'config/autoconf.mk')
             if os.path.isfile(_path):
                 path = _path
                 break
 
         if path == None:
-            print ("Can't find config/autoconf.mk on a directory containing the JS shell"
-                   " (searched from %s)") % jsdir
+            print(("Can't find config/autoconf.mk on a directory containing the JS shell"
+                   " (searched from %s)") % jsdir)
             sys.exit(1)
 
         # Read the values.
         val_re = re.compile(r'(TARGET_XPCOM_ABI|OS_TARGET|MOZ_DEBUG)\s*=\s*(.*)')
         kw = { 'isdebug': False }
         for line in open(path):
             m = val_re.match(line)
             if m:
@@ -161,17 +163,17 @@ def _parse_one(testcase, xul_tester):
             testcase.slow = True
             pos += 1
         elif parts[pos] == 'silentfail':
             # silentfails use tons of memory, and Darwin doesn't support ulimit.
             if xul_tester.test("xulRuntime.OS == 'Darwin'"):
                 testcase.expect = testcase.enable = False
             pos += 1
         else:
-            print 'warning: invalid manifest line element "%s"'%parts[pos]
+            print('warning: invalid manifest line element "%s"'%parts[pos])
             pos += 1
 
 def _build_manifest_script_entry(script_name, test):
     line = []
     if test.terms:
         line.append(test.terms)
     line.append("script")
     line.append(script_name)
diff --git a/js/src/tests/lib/results.py b/js/src/tests/lib/results.py
--- a/js/src/tests/lib/results.py
+++ b/js/src/tests/lib/results.py
@@ -1,8 +1,10 @@
+from __future__ import print_function
+
 import re
 from progressbar import NullProgressBar, ProgressBar
 import pipes
 
 # subprocess.list2cmdline does not properly escape for sh-like shells
 def escape_cmdline(args):
     return ' '.join([ pipes.quote(a) for a in args ])
 
@@ -120,20 +122,20 @@ class ResultsSink:
             show = self.options.show
             if self.options.failed_only and dev_label not in ('REGRESSIONS', 'TIMEOUTS'):
                 show = False
             if show:
                 self.pb.beginline()
 
             if show:
                 if self.options.show_output:
-                    print >> self.fp, '## %s: rc = %d, run time = %f' % (output.test.path, output.rc, output.dt)
+                    print('## %s: rc = %d, run time = %f' % (output.test.path, output.rc, output.dt), file=self.fp)
 
                 if self.options.show_cmd:
-                    print >> self.fp, escape_cmdline(output.cmd)
+                    print(escape_cmdline(output.cmd), file=self.fp)
 
                 if self.options.show_output:
                     self.fp.write(output.out)
                     self.fp.write(output.err)
 
             self.n += 1
 
             if result.result == TestResult.PASS and not result.test.random:
@@ -184,46 +186,47 @@ class ResultsSink:
         (TestResult.PASS,  False, False): ('TEST-UNEXPECTED-PASS',               'FIXES'),
         (TestResult.PASS,  False, True):  ('TEST-PASS (EXPECTED RANDOM)',        ''),
         (TestResult.PASS,  True,  False): ('TEST-PASS',                          ''),
         (TestResult.PASS,  True,  True):  ('TEST-PASS (EXPECTED RANDOM)',        ''),
         }
 
     def list(self, completed):
         for label, paths in sorted(self.groups.items()):
-            if label == '': continue
+            if label == '':
+                continue
 
-            print label
+            print(label)
             for path in paths:
-                print '    %s'%path
+                print('    %s' % path)
 
         if self.options.failure_file:
               failure_file = open(self.options.failure_file, 'w')
               if not self.all_passed():
                   if 'REGRESSIONS' in self.groups:
                       for path in self.groups['REGRESSIONS']:
-                          print >> failure_file, path
+                          print(path, file=failure_file)
                   if 'TIMEOUTS' in self.groups:
                       for path in self.groups['TIMEOUTS']:
-                          print >> failure_file, path
+                          print(path, file=failure_file)
               failure_file.close()
 
         suffix = '' if completed else ' (partial run -- interrupted by user)'
         if self.all_passed():
-            print 'PASS' + suffix
+            print('PASS' + suffix)
         else:
-            print 'FAIL' + suffix
+            print('FAIL' + suffix)
 
     def all_passed(self):
         return 'REGRESSIONS' not in self.groups and 'TIMEOUTS' not in self.groups
 
     def print_tinderbox_result(self, label, path, message=None, skip=False, time=None):
         result = label
         result += " | " + path
         result += " |" + self.options.shell_args
         if message:
             result += " | " + message
         if skip:
             result += ' | (SKIP)'
         if time > self.options.timeout:
             result += ' | (TIMEOUT)'
-        print result
+        print(result)
 
diff --git a/js/src/tests/lib/tasks_unix.py b/js/src/tests/lib/tasks_unix.py
--- a/js/src/tests/lib/tasks_unix.py
+++ b/js/src/tests/lib/tasks_unix.py
@@ -140,17 +140,17 @@ def reap_zombies(tasks, results, timeout
     results channel.  This method returns a new task list that has had the ended
     tasks removed.
     """
     while True:
         try:
             pid, status = os.waitpid(0, os.WNOHANG)
             if pid == 0:
                 break
-        except OSError, e:
+        except OSError as e:
             if e.errno == errno.ECHILD:
                 break
             raise e
 
         ended = remove_task(tasks, pid)
         flush_input(ended.stdout, ended.out)
         flush_input(ended.stderr, ended.err)
         os.close(ended.stdout)
diff --git a/js/src/tests/lib/tasks_win.py b/js/src/tests/lib/tasks_win.py
--- a/js/src/tests/lib/tasks_win.py
+++ b/js/src/tests/lib/tasks_win.py
@@ -1,10 +1,12 @@
 # Multiprocess activities with a push-driven divide-process-collect model.
 
+from __future__ import print_function
+
 from threading import Thread, Lock
 from Queue import Queue, Empty
 from datetime import datetime
 
 class Source:
     def __init__(self, task_list, results, timeout, verbose = False):
         self.tasks = Queue()
         for task in task_list:
@@ -14,22 +16,24 @@ class Source:
         self.timeout = timeout
         self.verbose = verbose
 
     def start(self, worker_count):
         t0 = datetime.now()
 
         sink = Sink(self.results)
         self.workers = [ Worker(_+1, self.tasks, sink, self.timeout, self.verbose) for _ in range(worker_count) ]
-        if self.verbose: print '[P] Starting workers.'
+        if self.verbose:
+            print('[P] Starting workers.')
         for w in self.workers:
             w.t0 = t0
             w.start()
         ans = self.join_workers()
-        if self.verbose: print '[P] Finished.'
+        if self.verbose:
+            print('[P] Finished.')
         return ans
 
     def join_workers(self):
         try:
             for w in self.workers:
                 w.join(20000)
             return True
         except KeyboardInterrupt:
@@ -61,17 +65,17 @@ class Worker(Thread):
 
         self.thread = None
         self.stop = False
 
     def log(self, msg):
         if self.verbose:
             dd = datetime.now() - self.t0
             dt = dd.seconds + 1e-6 * dd.microseconds
-            print '[W%d %.3f] %s' % (self.id, dt, msg)
+            print('[W%d %.3f] %s' % (self.id, dt, msg))
 
     def run(self):
         try:
             while True:
                 if self.stop:
                     break
                 self.log('Get next task.')
                 task = self.tasks.get(False)
diff --git a/js/src/tests/parsemark.py b/js/src/tests/parsemark.py
--- a/js/src/tests/parsemark.py
+++ b/js/src/tests/parsemark.py
@@ -16,16 +16,18 @@ Effectively, a bad run from the current 
 baseline data, we're probably faster. A similar computation is used for
 determining the "slower" designation.
 
 Arguments:
   shellpath             executable JavaScript shell
   dirpath               directory filled with parsilicious js files
 """
 
+from __future__ import print_function
+
 import math
 import optparse
 import os
 import subprocess as subp
 import sys
 from string import Template
 
 try:
@@ -72,39 +74,39 @@ def bench(shellpath, filepath, warmup_ru
     code = JS_CODE_TEMPLATE.substitute(filepath=filepath,
             warmup_run_count=warmup_runs, real_run_count=counted_runs)
     proc = subp.Popen([shellpath, '-e', code], stdout=subp.PIPE)
     stdout, _ = proc.communicate()
     milliseconds = [float(val) for val in stdout.split(',')]
     mean = avg(milliseconds)
     sigma = stddev(milliseconds, mean)
     if not stfu:
-        print 'Runs:', [int(ms) for ms in milliseconds]
-        print 'Mean:', mean
-        print 'Stddev: %.2f (%.2f%% of mean)' % (sigma, sigma / mean * 100)
+        print('Runs:', [int(ms) for ms in milliseconds])
+        print('Mean:', mean)
+        print('Stddev: %.2f (%.2f%% of mean)' % (sigma, sigma / mean * 100))
     return mean, sigma
 
 
 def parsemark(filepaths, fbench, stfu=False):
     """:param fbench: fbench(filename) -> float"""
     bench_map = {} # {filename: (avg, stddev)}
     for filepath in filepaths:
         filename = os.path.split(filepath)[-1]
         if not stfu:
-            print 'Parsemarking %s...' % filename
+            print('Parsemarking %s...' % filename)
         bench_map[filename] = fbench(filepath)
-    print '{'
+    print('{')
     for i, (filename, (avg, stddev)) in enumerate(bench_map.iteritems()):
         assert '"' not in filename
         fmt = '    %30s: {"average_ms": %6.2f, "stddev_ms": %6.2f}'
         if i != len(bench_map) - 1:
             fmt += ','
         filename_str = '"%s"' % filename
-        print fmt % (filename_str, avg, stddev)
-    print '}'
+        print(fmt % (filename_str, avg, stddev))
+    print('}')
     return dict((filename, dict(average_ms=avg, stddev_ms=stddev))
             for filename, (avg, stddev) in bench_map.iteritems())
 
 
 def main():
     parser = optparse.OptionParser(usage=__doc__.strip())
     parser.add_option('-w', '--warmup-runs', metavar='COUNT', type=int,
             default=5, help='used to minimize test instability [%default]')
@@ -117,35 +119,35 @@ def main():
             'compare against')
     parser.add_option('-q', '--quiet', dest='stfu', action='store_true',
             default=False, help='only print JSON to stdout [%default]')
     options, args = parser.parse_args()
     try:
         shellpath = args.pop(0)
     except IndexError:
         parser.print_help()
-        print
-        print >> sys.stderr, 'error: shellpath required'
+        print()
+        print('error: shellpath required', file=sys.stderr)
         return -1
     try:
         dirpath = args.pop(0)
     except IndexError:
         parser.print_help()
-        print
-        print >> sys.stderr, 'error: dirpath required'
+        print()
+        print('error: dirpath required', file=sys.stderr)
         return -1
     if not shellpath or not os.path.exists(shellpath):
-        print >> sys.stderr, 'error: could not find shell:', shellpath
+        print('error: could not find shell:', shellpath, file=sys.stderr)
         return -1
     if options.baseline_path:
         if not os.path.isfile(options.baseline_path):
-            print >> sys.stderr, 'error: baseline file does not exist'
+            print('error: baseline file does not exist', file=sys.stderr)
             return -1
         if not compare_bench:
-            print >> sys.stderr, 'error: JSON support is missing, cannot compare benchmarks'
+            print('error: JSON support is missing, cannot compare benchmarks', file=sys.stderr)
             return -1
     benchfile = lambda filepath: bench(shellpath, filepath,
             options.warmup_runs, options.counted_runs, stfu=options.stfu)
     bench_map = parsemark(gen_filepaths(dirpath), benchfile, options.stfu)
     if options.baseline_path:
         compare_bench.compare_immediate(bench_map, options.baseline_path)
     return 0
 
